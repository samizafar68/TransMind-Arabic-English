# ğŸŒ Arabic to English Neural Machine Translator | Transformer from Scratch

A powerful, minimal, and educational implementation of a **Neural Machine Translation (NMT)** system that translates **Arabic â English** using a **Transformer-based Seq2Seq model**, built completely **from scratch** in PyTorch â€” no pretrained models or external tokenizers used.

Deployed with an intuitive **Streamlit Web App** for live interaction and demonstration.

---

## ğŸ“Œ Features

âœ… Transformer architecture (Encoder-Decoder)  
âœ… Custom Tokenizer (Word-level with special tokens)  
âœ… Trained on Tatoeba (Helsinki-NLP) Arabic-English dataset  
âœ… Greedy decoding for inference  
âœ… Clean Streamlit app interface for live translation  
âœ… Easy to extend, customize, and learn from

---

## ğŸš€ Demo

Try it yourself:

ğŸ”— **[Live Streamlit App](#)** *https://transmind-arabic-english-h9pqu2tvwjjqdxeq4jan3s.streamlit.app/*
ğŸ”— **[See On Linkedln](#)** *https://transmind-arabic-english-h9pqu2tvwjjqdxeq4jan3s.streamlit.app/*
ğŸ”— **[See Medium Blog](#)** *https://transmind-arabic-english-h9pqu2tvwjjqdxeq4jan3s.streamlit.app/*

---

## ğŸ§  Concepts Used

### âœ¨ Transformer (Seq2Seq) â€“ From Scratch
- Encoder-Decoder architecture using `torch.nn.Transformer`
- Positional Encoding to retain sequence order
- Multi-head attention and feed-forward layers
- Greedy decoding loop for generating output

### âœ¨ Tokenization
- Custom word-level tokenizer
- Handles Arabic and English text
- Special token handling: `<sos>`, `<eos>`, `<pad>`, `<unk>`

### âœ¨ Dataset
- **Helsinki-NLP / Tatoeba** dataset via HuggingFace `datasets` library
- Combined `validation` and `test` sets for training

---

## ğŸ§ª Model Architecture

```text
[Arabic Text] â†’ Tokenizer â†’ Embedding + Positional Encoding â†’ Encoder â†’ Decoder â†’ Linear Layer â†’ Softmax â†’ [English Translation]
